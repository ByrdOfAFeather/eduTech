{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preface\n",
    "This notebook is a means of documenting why this module behaves the way it does in ways that simply won't \n",
    "fit into the actual code of the module. It is not a necessary read to use the module, but if you find that you are \n",
    "seeing inconsistent results with what you expected, please read through this to understand the models that are implemented \n",
    "and how they are implemented. \n",
    "\n",
    "# 1. Implementing a RaschModel\n",
    "\n",
    "## 1.1: JAG Model Description\n",
    "An important question anybody has to ask when trying to implement something is what exactly is that something \n",
    "that they are trying to implement? In my case, it was: \"what exactly is a Rasch model?\" When I started this project \n",
    "my initial goal was to implement a Cornell paper, *JAG: Joint Assessment Grading*. In this paper, they defined the Rasch model\n",
    "as \n",
    "    \n",
    "$$ P(X_{ij} = 1 | S_i, Q_j) = \\frac{1}{1+e^{-X_{ij} * (S_i - Q_j)}} \\tag{1}$$ \n",
    "\n",
    "where $X_{ij}$ is an encoded answer, either 1 or 0 on from some\n",
    "sort of questionnaire. In the case of the paper, it was in the context of an academic test. Thus, $S_i$ was The $i^{th}$ student's \n",
    "ability, $Q_J$ the $j^{th}$ question's difficulty, and $X_{ij} = 1 $ represented a correct answer while $X_{ij} = 0 $ represented\n",
    "an incorrect answer. The estimation of the student ability and question difficulty (according to the JAG paper)\n",
    "is done through maximizing a likelihood function that is described in the original paper. In practice a negative log likelihood is\n",
    "minimized which is not defined in the JAG paper.\n",
    "\n",
    "(Studer) \n",
    "\n",
    "## 1.2: Why Explore other descriptions for the Rasch model? \n",
    "When I was implementing the Rasch model, I needed to find something to test my implementation against. Initially, following the JAG \n",
    "paper, I implemented the Rasch model by maximizing the log likelihood of the model. \n",
    "\n",
    "---\n",
    "### 1.2.1: Side note on Rasch Model Optimization\n",
    "In reality I was minimizing the negative log likelihood with tensorflow's \n",
    "automatic differentiation of the following loss function: \n",
    "\n",
    "$$loss = -\\sum_{i,j} X_{ij}\\log(\\sigma(-X_{ij}(S_i-Q_j)) + (1-X_{ij})\\log(1-\\sigma(-X_{ij}(S_i-Q_j)))$$  \n",
    "\n",
    "This loss function is common in logistic regression but has no academic citation since the majority of papers exploring the Rasch\n",
    "model either do not define a loss function for logistic regression in its entirety (A great example of \n",
    "this is a paper from SAS on using logistic regression to optimize the Rasch model which \n",
    "defines seperate loss functions for the two parameters but does not define how to apply these \n",
    "functions in a way to optimize both parameters), or don't use \n",
    "logistic regression at all. In the latter case, Marginal Maximum Likelihood or Bayesian estimation techniques are used.\n",
    "The lack of clarity in the Rasch model's optimization was one of the motivations for \n",
    "exploring further model descriptions. \n",
    "\n",
    "---\n",
    "\n",
    "Eventually, I came across a tutorial on pyschometric parameter estimation from Penn State's Social Science Research Institute. \n",
    "This tutorial can be found here: https://quantdev.ssri.psu.edu/tutorials/introduction-irt-modeling. What's notable about this \n",
    "tutorial is the following lines:\n",
    "\n",
    "```\n",
    "\"The 1PL (also called the Rasch model) IRT model describes test items in terms of only one parameter, item difficulty, q. Item difficulty is simply how hard an item is (how high does the latent trait ability level need to be in order to have a 50% chance of getting the item right?). q is estimated for each item of the test.\"\n",
    "```\n",
    "\n",
    "The Rasch model this tutorial puts forth is defined as \n",
    "$$ P(X_{ij} = 1 | S_i, Q_j) =  \\frac{e^{S_i - Q_j}}{1+e^{S_i - Q_j}} \\tag{2}$$\n",
    "\n",
    "Now a inconsistency has been introduced. Thus, investigation into more descriptions needs to occur in an attempt to find which description\n",
    "can be trusted. \n",
    "\n",
    "\n",
    "Note: Equation (2)'s syntax is changed from the tutorial based on my interpretation of the Penn State parameters in terms of the original JAG implementation, \n",
    "in particular, $b = Q$ and $\\theta = S$. This will remain the same for future equations. \n",
    "\n",
    "\n",
    "## 1.3: Chris Hulme-Lowe Dissertation Model Description\n",
    "While investigating this issue, I came across a dissertation that described IRT parameter estimation and optimization methods in great \n",
    "detail. This paper presented the Rasch model as different from the 1PL (1 parameter logistic model), \n",
    "\"The Rasch model is less restrictive than the 1PL, but more restrictive than the 2PL,\" (Hulme-Lowe). This is in contrast to \n",
    "the Penn State tutorial which presented the Rasch model as equivalent to the 1PL, which can be seen in the previous section's\n",
    "excerpt. Still, the dissertation presents the Rasch\n",
    "model nearly identicalally to the Penn State description:\n",
    "\n",
    "$$ P(X_{ij} = 1 | S_i, Q_j) =  \\frac{\\exp(Da_j(S_i - Q_j))}{1+\\exp(Da_j(S_i - Q_j))} \\tag{3} $$\n",
    "\n",
    "Where, as explained in the paper, D can be thought of as 1. In fact, in most Rasch model software implementations D is set to 1, \n",
    "\"However, the logistic model has become so ubiquitous in IRT that many modern software packages leave D = 1, \n",
    "which produces parameters in the logistic metric,\" (Hulme-Lowe). As well, $a_j$ is the $j^{th}$ discrimination parameter (a question's ability to determine student ability)\n",
    "and for the Rasch model $a_j = \\bar{a}$ and typically $ \\bar{a} = 1 $ \n",
    "\n",
    "(However, $\\bar{a}$'s value is not discussed by Hulme-Lowe, this information was gathered from\n",
    "the [R implementation](https://www.rdocumentation.org/packages/ltm/versions/1.1-1/topics/rasch) of the Rasch model, \"Although the common formulation of the Rasch model assumes that the discrimination parameter is fixed to 1...\").\n",
    "\n",
    "Thus when simplified the Rasch model as described by Hulme-Lowe is\n",
    "\n",
    "$$ P(X_{ij} = 1 | S_i, Q_j) =  \\frac{\\exp(S_i - Q_j)}{1+\\exp(S_i - Q_j)} \\tag{4} $$\n",
    "\n",
    "And thus, (4) = (3) = (2), in theory. So from this I can only extrapolate that the Rasch must be equivalent to the 1PL when all of the \n",
    "parameters that make the Rasch model distinct are set to 1. The problem being that Penn State claims that only difficulty parameters are estimated\n",
    "in this case. Yet, the JAG paper asserts that the Rasch model allows the estimation of both student ability and question \n",
    "difficulty, \"This allows us to estimate the ability of each student and the difficulty of each question by maximizing the\n",
    "likelihood of all outcomes under our model,\" (Hulme-Lowe).\n",
    "\n",
    "---\n",
    "### 1.3.1: Side Note: Attempting to optimize the Hulme-Lowe 2PL\n",
    "Hulme-Lowe puts forth multiple methods of optimization in his dissertation. One of them is Marginal Maximum Likelihood estimation.\n",
    "I attempted to implement this for the 2PL model that he presents (which seems to be closest to the JAG description since it \n",
    "estimates student ability and question difficulty) but always had a problem with run away gradients that ended up producing \n",
    "a lot of NaN values in the e-step. My work on this implementation\n",
    "can be found [here](https://gist.github.com/ByrdOfAFeather/56b5fac75d9fee36884f2f9c12ba8ce2). Notably, I set D = 1.702 \n",
    "since that is how the dissertation uses it (I do not know why). \n",
    "\n",
    "The fact that I couldn't implement this in a working fashion was part of the motivation to find other descriptions of the model.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4: A Quick Look At Internet Resources\n",
    "At this point I was quite confused, as you might imagine, how a paper from Cornell that builds off the Rasch model managed \n",
    "to both define the Rasch model so very wrong and get published with that mistake completely uncaught. At least, at this time I was \n",
    "under the impression that it was defined incorrectly.\n",
    "So I decided to do some more investigation. First I started by trying to find an already existing Python implementation of the Rasch model\n",
    "as described by both the Penn State tutorial and Hulme-Lowe. \n",
    "As far as I am aware, this does not exist. I did, however, come across these sources: \n",
    "\n",
    "### 1.4.1: A tutorial on how to implement the Rasch model in Tensorflow:\n",
    "[Source](https://www.kaggle.com/mlarionov/the-rasch-model). This implementation the Rasch model is very close to what\n",
    "is described by the original JAG paper. However, of note, the multiplication of the exponent by $-X_{ij}$ in the sigmoid \n",
    "function is missing. This is a significant factor as the probability that a student got a question correct is always .5 if \n",
    "they actually got it wrong when the $-X_{ij}$ is present, which, as far as I am aware that is correct only under the assumption that \n",
    "when a student gets a question wrong it's because they guessed with no prior knowledge or ability. What's most confusing about this source is the fact that now we have a ***THIRD*** implementation\n",
    "of the Rasch model.\n",
    "\n",
    "i.e. (22.1)\n",
    "\n",
    "$$ P(X_{ij} = 1 | S_i, Q_j) = \\frac{1}{1+e^{(S_i - Q_j)}} $$\n",
    "\n",
    "However, this is technically equivalent to the Hulme-Lowe when \n",
    "\n",
    "$$\\bar{a} = \\frac{1}{e^{(S_i - Q_j)}}$$\n",
    "\n",
    "### 1.4.2: David Barber's *Bayesian Reasoning and Machine Learning*\n",
    "[Source](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090310.pdf).\n",
    "Where did the previous description of Rasch come from? Well, it seems as credible as any other source. It comes from a book\n",
    "titled *Bayesian Reasoning and Machine Learning* by David Barber, a professor at the University College of London. Who \n",
    "defines the Rasch model (5) and claims that, in fact, both student ability and question difficulty are estimated\n",
    "by the Rasch model (22.1). \n",
    "\n",
    "### 1.4.3: Could Penn State's tutorial be incorrect? \n",
    "\n",
    "[Source](https://github.com/lukas-a-olson/item-response-theory/). According to the python notebook of this individual \n",
    "also attempting to follow the Penn State tutorial, there is some sort of incorrectness in their visualizations. \"This \n",
    "clearly indicates an error in the tutorial's visualization: it should peak at 0.25, not 0.5. That lends credibility to \n",
    "my analysis, even though I still do not fully understand the model fit.\" Though, this is only a problem in visualization \n",
    "and may or may not reflect on the model. \n",
    "\n",
    "Which leaves me with the following \n",
    "\n",
    "- JAG's Rasch Model \n",
    "- Penn State's Tutorial/Hulme-Lowe's Rasch Model \n",
    "- David Barber's Rasch Model\n",
    "\n",
    "and not a single one of them seems to be more or less correct than the others.\n",
    "\n",
    "## 1.5: Do the various models produce the \"same\" results? \n",
    "\n",
    "So then, what if all the models are, in fact, correct? After all the Rasch model is simply a function that has been developed \n",
    "to behave in a way that provides artificial constraints to student ability based on question difficulty. None of the previously \n",
    "defined models really deviate from that, they simply do it on slightly different scales, as far as I am aware. While the \n",
    "numeric values are going to be different, perhaps the parameter ordering is the same (i.e. student one has the highest student \n",
    "ability even if it varies from 10 to .01). \n",
    "\n",
    "The data set is that I'm going to test on is found in the Penn State tutorial. It's an example representing a test\n",
    "with 500 students and 10 questions. For this we will start with looking at question difficulties, if inconsistencies can\n",
    "be found there, looking at all 500 students in the test dataset won't be necessary. \n",
    "\n",
    "\n",
    "### 1.5.1: David Barber's Implementation (eduTech's Implementation)\n",
    "For more information on why this is known to be a correct implementation of David Barber's description see the tests/RaschModelTests.ipynb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "LOSS AFTER 0 ITERATIONS: -3465.7294921875\nLOSS AFTER 100 ITERATIONS: -2812.181396484375\nLOSS AFTER 200 ITERATIONS: -2759.679443359375\n",
      "LOSS AFTER 300 ITERATIONS: -2711.19921875\nLOSS AFTER 400 ITERATIONS: -2666.448974609375\n",
      "LOSS AFTER 500 ITERATIONS: -2625.1435546875\nFinished fitting with a final loss of -2587.384765625\n",
      "======= Question Difficulties =======\n[[1.7579474  1.0194758  0.7740919  0.879095   0.2520845  0.7928815\n  0.36027655 0.7001337  0.77409184 2.6160252 ]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from irt.rasch import RaschModel\n",
    "def load_test_data():\n",
    "    with open(\"tests/ouirt.txt\", \"r\") as f:\n",
    "        rows = []\n",
    "        for line in f.readlines():\n",
    "            current_row = []\n",
    "            for no in line.split(\" \"):\n",
    "                try:\n",
    "                    current_row.append(int(no))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            rows.append(current_row)\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# The learning rate and epochs are arbitrary and chosen to match the likelihood of \n",
    "# the Penn State tutorial, in reality the R implementation use Marginal Maximum likelihood estimation \n",
    "# which optimizes the model in a significantly different way. \n",
    "test_model = RaschModel(learning_rate=.1)\n",
    "test_data = load_test_data()\n",
    "test_model.fit(test_data, epochs=600)\n",
    "student_abilities, question_difficulties = test_model.get_model_descriptors()\n",
    "print(\"======= Question Difficulties =======\")\n",
    "print(question_difficulties)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5.2: Penn State/Hulme-Lowe's Implementation\n",
    "\n",
    "This model is already implemented in R and in the Penn State tutorial, they use the R implementation, thus \n",
    "the results are as they are presented in the tutorial.\n",
    "\n",
    "```\n",
    "## Model Summary:\n",
    "##    log.Lik      AIC      BIC\n",
    "##  -2587.186 5196.372 5242.733\n",
    "## \n",
    "## Coefficients:\n",
    "##             value std.err  z.vals\n",
    "## Dffclt.V1  1.6581  0.1303 12.7219\n",
    "## Dffclt.V2  0.9818  0.1031  9.5233\n",
    "## Dffclt.V3  0.7499  0.0968  7.7431\n",
    "## Dffclt.V4  0.8495  0.0993  8.5522\n",
    "## Dffclt.V5  0.2482  0.0891  2.7851\n",
    "## Dffclt.V6  0.7677  0.0973  7.8930\n",
    "## Dffclt.V7  0.3528  0.0900  3.9186\n",
    "## Dffclt.V8  0.6794  0.0953  7.1311\n",
    "## Dffclt.V9  0.7499  0.0968  7.7429\n",
    "## Dffclt.V10 2.3978  0.1764 13.5892\n",
    "## Dscrmn     1.3782  0.0741 18.5941\n",
    "```\n",
    "\n",
    "### 1.5.3: JAG's Implementation\n",
    "\n",
    "JAG's implementation is a bit more tricky. The model doesn't technically exist already. So a full-proof implementation \n",
    "also doesn't quite exist. I will detail what assumptions I made in this implementation and how I implemented it in the hopes \n",
    "that if it's correct then it's well documented, or if not, it can be corrected. \n",
    "\n",
    "The loss function:\n",
    "\n",
    "$$loss = \\sum_{i,j} X_{ij}\\log(\\sigma(-X_{ij}(S_i-Q_j)) + (1-X_{ij})\\log(1-\\sigma(-X_{ij}(S_i-Q_j)))$$  \n",
    "\n",
    "See 1.2.1 to get some idea on where this comes from, notably, this is the same loss function that is used in the David Baber\n",
    "implementation aside from the added $-X_{ij}$. \n",
    "\n",
    "With the following derivatives:\n",
    "\n",
    "$$\\frac{\\partial{loss}}{\\partial{S_i}} = \\sum_{j} X_{ij} - \\sigma(-X_{ij} * (S_i - Q_j))$$\n",
    "\n",
    "$$\\frac{\\partial{loss}}{\\partial{Q_j}} = -\\sum_{i} X_{ij} - \\sigma(-X_{ij} * (S_i - Q_j))$$\n",
    "\n",
    "Thus in Python:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "LOSS AFTER 0 ITERATIONS: -3465.7294921875\n",
      "LOSS AFTER 100 ITERATIONS: -2927.439208984375\n",
      "LOSS AFTER 200 ITERATIONS: -2625.4560546875\nFinished fitting with a final loss of -2551.02734375\n======= Question Difficulties =======\n[[5.040559  3.9364557 3.3865798 3.6371763 1.567964  3.433293  2.0597837\n  3.1937835 3.3865442 5.7031174]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from irt.rasch import RaschModel\n",
    "\n",
    "class JAGRachModel(RaschModel):\n",
    "    def __init__(self, learning_rate):\n",
    "        RaschModel.__init__(self, learning_rate)\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict(batch_students, batch_questions, results):\n",
    "        return tf.sigmoid(-results * (batch_students - batch_questions))\n",
    "    \n",
    "    def _train(self, calc_loss=False):\n",
    "        predictions = self.predict(self.student_abilities, self.questions_difficulties, self.results)\n",
    "        student_ability_gradient = self.learning_rate * self._calc_deriv_student_ability(predictions)\n",
    "        student_ability_gradient = student_ability_gradient / student_ability_gradient.shape[0]\n",
    "        \n",
    "        question_difficulties_gradient = self.learning_rate * self._calc_deriv_question_difficulty(predictions)\n",
    "        question_difficulties_gradient = question_difficulties_gradient / question_difficulties_gradient.shape[1]\n",
    "        \n",
    "        self.student_abilities.assign_add(student_ability_gradient)\n",
    "        self.questions_difficulties.assign_add(question_difficulties_gradient)\n",
    "        \n",
    "        if calc_loss:\n",
    "            return self._calc_loss(predictions, self.results)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "def load_test_data():\n",
    "    with open(\"tests/ouirt.txt\", \"r\") as f:\n",
    "        rows = []\n",
    "        for line in f.readlines():\n",
    "            current_row = []\n",
    "            for no in line.split(\" \"):\n",
    "                try:\n",
    "                    current_row.append(int(no))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            rows.append(current_row)\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# The learning rate and epochs are arbitrary and chosen to match the likelihood of \n",
    "# the Penn State tutorial, in reality the R implementation use Marginal Maximum likelihood estimation \n",
    "# which optimizes the model in a significantly different way. \n",
    "test_model = JAGRachModel(learning_rate=.001)\n",
    "test_data = load_test_data()\n",
    "test_model.fit(test_data, epochs=250)\n",
    "student_abilities, question_difficulties = test_model.get_model_descriptors()\n",
    "print(\"======= Question Difficulties =======\")\n",
    "print(question_difficulties)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5.4: Testing the results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Value's order checks out!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "question_difficulties_david_barber = [1.7579474, 1.019475, 0.7740919, 0.879095, 0.2520845, 0.7928815, \n",
    "                                      0.36027655, 0.7001337, 0.77409184, 2.6160252 ]\n",
    "question_difficulties_penn_state = [1.6581, .9818, .7499, .8495, .2482, .7677, .3528, .6794, .7499, 2.3978]\n",
    "question_difficulties_JAG = [5.040559, 3.9364557, 3.3865798, 3.6371763, 1.567964, 3.433293, 2.0597837, 3.1937835, \n",
    "                             3.3865442, 5.7031174]\n",
    "\n",
    "\n",
    "sorted_indexes_question_david_barber = np.argsort(question_difficulties_david_barber)\n",
    "sorted_indexes_question_penn_state = np.argsort(question_difficulties_penn_state)\n",
    "sorted_indexes_difficulties_JAG = np.argsort(question_difficulties_JAG)\n",
    "for current_question_index, index_david_barber in enumerate(sorted_indexes_question_david_barber):\n",
    "    if sorted_indexes_question_penn_state[current_question_index] == 2 or sorted_indexes_question_penn_state[current_question_index] == 8:\n",
    "        continue\n",
    "    \n",
    "    assert index_david_barber == sorted_indexes_difficulties_JAG[current_question_index] \\\n",
    "           and index_david_barber == sorted_indexes_question_penn_state[current_question_index], \\\n",
    "        f\"ERROR AT QUESTION {current_question_index}\\n\" \\\n",
    "        f\"Barber thinks that the current index should be {index_david_barber}\\n\" \\\n",
    "        f\"Penn State thinks that the current index should be {sorted_indexes_question_penn_state[current_question_index]}\\n\" \\\n",
    "        f\"JAG thinks that the current index should be {sorted_indexes_difficulties_JAG[current_question_index]}\" \n",
    "\n",
    "print(\"Value's order checks out!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5.5: Results\n",
    "\n",
    "One thing to note here is that the Penn State tutorial ended up having a model with a repeating question difficulty. This\n",
    "makes these points harder to compare, but the indices are the same as the other sorted lists\n",
    "same if the indices with the same value are swapped (8 with 2 and 2 with 8). Thus question difficulties are of the same \n",
    "meaning. We can infer from all models that Question 10 is a harder question than any other question and question 5 is easier than any other question, \n",
    "indicative that all models are in fact equivalent. \n",
    "\n",
    "There's still one caveat. ***Penn State doesn't have student abilities because of its assertion that the \n",
    "Rasch model does not estimate student abilities***. Even though the David Barber's description does optimize the \n",
    "student abilities and the resulting question difficulties are incredibly similar. \n",
    "\n",
    "## 1.6: Conclusion\n",
    "\n",
    "What is correct? The models are produce something that can be argued to be equivalent but why are they different in the \n",
    "first place? For optimization purposes? For different constraints? It's unclear. JAG and Berber both use logistic regression\n",
    "while Hulme-Lowe and Penn State both use Marginal Maximum Likelihood, however, as mentioned in 1.2.1, there's a paper \n",
    "that focuses entirely on logistic regression from SAS that defined the Rasch model in the same way that Hulme-Lowe does (Pan). \n",
    "Except in that paper, both student ability and question difficulty are estimated from that model (Pan).\n",
    "\n",
    "It seems as though there's a multitude of ways to actually define the Rasch model, each having a semantic equivalence but \n",
    "numeric difference. For eduTech's implementation, I've decided to implement the Berber description for the following \n",
    "reasons: \n",
    "\n",
    "- It's simple with clearly defined derivatives\n",
    "- It is the closest to the Penn State tutorial in terms of question difficulties, but also estimates student abilities\n",
    "- Compared to Marginal Maximum Likelihood estimation it's significantly less computationally expensive \n",
    "\n",
    "Future plans for the item response theory module of eduTech includes the 3PL and choosing methods of optimization. Other \n",
    "models could be implemented but I no longer know what the difference between the Berber-Rasch model and the 2PL is.\n",
    "\n",
    "About:\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Last Updated: 10/22/19\n",
    "\n",
    "---\n",
    "\n",
    "# BIBLIOGRAPHY \n",
    "\n",
    "“22.1 The Rasch Model.” Bayesian Reasoning and Machine Learning, by David Barber, University Cambridge Press, 2012, pp. 403–404.\n",
    "\n",
    "Hulme-Lowe, Chris. “Regularized Marginal Maximum Likelihood: The Use of Shrinkage and Selection Operators for Item Parameter Estimation in the Two-Parameter Logistic Model.”\n",
    "\n",
    "Mlarionov. “The Rasch Model.” Kaggle, Kaggle, 21 Oct. 2018, www.kaggle.com/mlarionov/the-rasch-model.\n",
    "\n",
    "Pan, Tianshu. “Fitting the Rasch Model under the Logistic Regression Framework to Reduce Estimation Bias.” \n",
    "Journal of Modern Applied Statistical Methods, vol. 17, no. 1, 2018, doi:10.22237/jmasm/1530028025.\n",
    "\n",
    "Studer, Christoph, and Igor Labutov. “JAG: A Crowdsourcing Framework for Joint Assessment and Peer Grading.” AAAI Publications, Thirty-First AAAI Conference on Artificial Intelligence, 12 Feb. 2017.\n",
    "\n",
    "Wood, Julie, and Peter v C.M. Molenaar. “Introduction to IRT Modeling.” Quantdev, 12 Nov. 2017, quantdev.ssri.psu.edu/tutorials/introduction-irt-modeling."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}